<!DOCTYPE html>
<html
  lang="en"
  class="no-js classic-page-layout"
  data-audio-wind="audio/wind.mp3"
  data-audio-wind-reverse="audio/wind-reverse.mp3"
  data-audio-tick="audio/tick.mp3"
>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Camille Dunning's Website" />
    <meta name="keywords" content="vcard, html5, portfolio" />
    <meta name="author" content="Camille Dunning" />
    <meta name="theme-color" content="#1755cf" />

    <title>Camille Dunning</title>

    <!-- FAV and TOUCH ICONS -->
    <link rel="shortcut icon" href="images/ico/icon.png" />
    <link rel="apple-touch-icon" href="images/ico/icon.png" />

    <!-- FONTS -->
    <link
      href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Poppins:300,300i,400,400i,600,600i,700,700i"
      rel="stylesheet"
    />

    <!-- STYLES -->
    <link rel="stylesheet" type="text/css" href="css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="js/nprogress/nprogress.css" />
    <link
      rel="stylesheet"
      type="text/css"
      href="js/jquery.magnific-popup/magnific-popup.css"
    />
    <link
      rel="stylesheet"
      type="text/css"
      href="css/fonts/fontello/css/fontello.css"
    />
    <link rel="stylesheet" type="text/css" href="css/align.css" />
    <link rel="stylesheet" type="text/css" href="css/layout.css" />
    <link rel="stylesheet" type="text/css" href="css/main.css" />
    <link rel="stylesheet" type="text/css" href="css/768.css" />

    <!-- INITIAL SCRIPTS -->
    <script src="js/jquery-3.2.1.min.js"></script>
  </head>

  <body>
    <!-- PAGE -->
    <div id="page" class="hfeed site">
      <!-- .site-main -->
      <main id="main" class="site-main cd-main">
        <!-- HEADER -->
        <header id="masthead" class="header" role="banner">
          <!-- header-wrap -->
          <div class="header-wrap layout-full">
            <img src="images/home/cam-dtn.jpg" alt="profile-image" />

            <h1 class="site-title">Camille Dunning</h1>
            <p class="site-description">Data Science Aspirant</p>

            <!-- header-social -->
            <div class="header-bottom">
              <a
                class="social-link facebook"
                href="https://www.facebook.com/camille.dunning/"
              ></a>
              <a
                class="social-link github"
                href="https://github.com/camille-004"
              ></a>
              <a
                class="social-link linkedin"
                href="https://www.linkedin.com/in/camille-d-780530129/"
              ></a>
              <a
                class="social-link instagram"
                href="https://www.instagram.com/okay.camille/"
              ></a>
            </div>
            <!-- header-social -->

            <!-- NAV MENU -->
            <nav
              id="primary-navigation"
              class="site-navigation primary-navigation"
              role="navigation"
            >
              <div class="nav-menu menu-with-icons">
                <ul>
                  <li><a class="home" href="index.html">Home</a></li>
                  <li><a class="return" href="blog.html">Back To Blog</a></li>
                </ul>
              </div>
            </nav>
            <!-- NAV MENU -->
          </div>
          <!-- header-wrap -->
        </header>
        <!-- HEADER -->

        <!-- site-middle -->
        <div class="site-middle">
          <div class="layout-fixed">
            <div id="primary" class="content-area">
              <!-- site-content -->
              <div id="content" class="site-content" role="main">
                <!-- .blog-single -->
                <div class="blog-single page-layout">
                  <!-- .hentry -->
                  <article class="post type-post hentry">
                    <!-- .entry-header -->
                    <header class="entry-header">
                      <h1 class="entry-title">
                        A Primer on Supervised and Unsupervised Machine Learning Models
                      </h1>

                      <!-- .entry-meta -->
                      <div class="entry-meta">
                        <span class="entry-date">
                          <time
                            class="entry-date"
                            datetime="2020-04-28T04:34:10+00:00"
                            >November 26, 2020</time
                          >
                        </span>
                      </div>
                      <!-- .entry-meta -->
                    </header>
                    <!-- .entry-header -->
                    
                    <p>As an aspiring machine learning engineer, I always tell people that I like to make computers think like humans.</p>

                    <!-- .featured-image -->
                    <div class="featured-image">
                      <img src="images/blog/ml-blog/cover.png" alt="blog-image" />
                    </div>
                    <!-- .featured-image -->

                    <!-- .entry-content -->
                    <div class="entry-content">

                      <p class="drop-cap">
                        ‚Ä¶Okay, I knew I couldn‚Äôt fool you. To tell you the truth, what we do isn‚Äôt anything fancy, and it‚Äôs certainly not magic. This isn‚Äôt to say that what ML can accomplish isn‚Äôt remarkable: these mathematical models optimize our internet searches, drive cars, re-tailor education experiences for all sorts of individuals, predict sicknesses, and classify all objects with uncanny accuracy, breaking milestones at prophetic rates.
                      </p>

                      <p>
                        Academics tout their ‚ÄúAI solutions‚Äù as homologs to human reasoning, but the merit of doing so doesn‚Äôt really transcend the fact that these algorithms work by learning and improving based on previous iterations. We operate on introspection, planning, higher-order thinking, and self-reflection. We take in sequential information well. The machines that we train hone <a href="https://www.datasciencecentral.com/profiles/blogs/understanding-the-applications-of-probability-in-machine-learning">probability</a>, an ability to take in feedback on a continuous, cyclical basis, and pattern recognition. Much of the time, the ‚Äúartificial mind‚Äù crunches numbers and ‚Äúthings‚Äù in a manner that remains a mystery to the general public.
                      </p>

                      <p>
                        So, what processes do ML algorithms undertake to achieve such eery results? They build internal models of the data (sensory information) and look for an underlying structure, sewing patterns from the top down. This is what is called an <strong>unsupervised learning</strong> task. Others take a bottom-up approach and generate their own assumptions about their outputs based on the correct answers they have learned before, similar to how we are taught in a classroom. This is the heart of <strong>supervised learning</strong>. Of course, these different workflows are saturated with moving parts beyond what most of us can comprehend, but the fundamental goals may not be as worthy of the reverence we hold them in.
                      </p>
                      
                      <p>
                        I will be going over the mechanics of some elementary machine learning models and differentiating between the unsupervised and supervised ones, and discussing why each of them falls into their respective categories. Let‚Äôs start with some supervised algorithms.
                      </p>
                      
                      <h2>Supervised Learning üü¢</h2>
                      
                      <h3>Linear Regression</h3>
                      
                      <p class="drop-cap">In simple terms, linear regression finds the best-fit line for describing the relationships between two or more variables. This is found by calculating the sum of the squared residuals from the observations to the predicted line. When we end up with a resultant line, we can express it as follows:</p>
                      
                      <p><i>E</i>(<i>Y</i> | <i>X_1</i>,‚Ä¶, <i>X_p</i>)=<i>Y</i>=<i>Œ≤_0</i>+<i>Œ≤_1X_1</i>+‚Ä¶+<i>Œ≤_pX_p</i>, where:</p>
                      
                      <p>
                        <ul>
                          <li><i>E</i> is the expected value.</li>
                          <li><i>Y</i> is the dependent variable, eventually designated as a function of the predictors.</li>
                          <li><i>X_i</i>, <i>i</i> = 1, ‚Ä¶, <i>p</i> are the observations.</li>
                          <li><i>Œ≤_i</i>,<i>i</i> = 1, ‚Ä¶, <i>p</i> are the degrees in change of the outcome for every unit of change in the predictor variable.</li>
                          <li><i>Œµ</i> is the disturbance, which represents variation in Y that is not caused by the predictors</li>
                        </ul>
                      </p>
                      
                      <p>
                        Note that a linear regression model has three main types of assumptions:
                        <ol>
                          <li><strong>Linearity assumption</strong>: There is a linear relationship between the dependent and independent variables.</li>
                          <li><strong>Error term assumptions</strong>: <i>Œµ</i>^(<i>i</i>) are normally distributed, independent of each other, have a mean of zero, and have constant variance, <i>œÉ</i>^2 (homoscedasticity).</li>
                          <li><strong>Estimator assumptions</strong>: Independent variables are independent of each other (no multicollinearity) and measured without error.</li>
                        </ol>
                      </p>
                      
                      <p>
                        Linear regression is supervised üü¢. You are trying to predict a real number from the model you trained on your dataset of known dependent variables.
                      </p>
                      
                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/lr.png" alt="Image" />
                        <p class="wp-caption-text">
                          Visual from Backlog
                        </p>
                      </div>
                      <!-- image aligncenter -->
                      
                      <h3>Linear Support Vector Machines (SVMs)</h3>
                      
                      <p class="drop-cap">SVMs detect classes in training data by finding the optimal hyperplane between the classes. This hyperplane must be optimal in the sense that it maximizes the margin, or separation, between the classes. This is used in both classification and regression problems, but SVM typically exemplifies the idea of a large margin classifier.</p>
                      
                      <p>The geometric intuition behind classifier optimality is shown below. There are clearly infinitely many separators for the two classes (blue circles and X‚Äôs), but there is one that is equidistant from the two ‚Äúblobs‚Äù. Therefore, we margin between the blue circles and the X‚Äôs is maximized. As for how this hyperplane is found, we take a subset of the training samples such that its elements are close to the decision surface, regardless of the class. Intuitively, SVM draws two parallel lines through the subset members of each class. These two parallel lines, in red below, are called support vectors.</p>
                      
                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/svm.png" alt="Image" />
                        <p class="wp-caption-text">
                          Image created by author
                        </p>
                      </div>
                      <!-- image aligncenter -->
                      
                      <p>
                        Eventually, the margin is maximized, and the line is drawn with the help of the support vectors. This optimal hyperplane will improve the classifier accuracy on new data points.
                      </p>
                      
                      <p>
                        In our case, the dataset is linearly separable and has no noise or outliers, making it a <strong>hard margin SVM</strong>. However, <strong>soft-margin SVMs</strong> are preferred in practice because they are less susceptible to overfitting than are hard-margin SVMs and are more versatile as they can include some misclassified outliers.
                      </p>
                      
                      <p>
                        Clearly, SVM is supervised üü¢. It requires the dataset to be fully labelled, hence the blue circles and the X‚Äôs. The only answer you‚Äôre trying to procure from SVM is the function describing the separation between the two known classes.
                      </p>

                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/svm-2.png" alt="Image" />
                        <p class="wp-caption-text">
                          Image from Velocity Business Solutions
                        </p>
                      </div>
                      <!-- image aligncenter -->
                      
                      <h3>Naive Bayes</h3>

                      <p class="drop-cap">
                        You probably understand the Naive Bayes operates with Bayes‚Äô Theorem, which gives you the posterior probability of an event given prior knowledge. We mathematically define it as follows:
                      </p>

                      <p>
                        P(<i>A</i>|<i>B</i>)=P(<i>B</i>|<i>A</i>)P(<i>A</i>) / P(<i>B</i>), where <i>i</i>, <i>B</i> =events
                      </p>
                      
                      <p>
                        It‚Äôs also expressed as the true positive rate of an experiment divided by the sum of the false positive rate of a population and the true positive rate of the experiment. What‚Äôs interesting about Bayes‚Äô Theorem is that it separates a test for an event from the event itself. You can read more about how this works <a href="https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/">here</a>.
                      </p>
                      
                      <p>
                        Naive Bayes is a binary and a multi-class classification algorithm in which predictions are simply made from calculating the probabilities of each data record associated with a particular class. The class having the largest probability for a data record is rated its most suitable class. Naive Bayes is an uncomplicated and fast algorithm and is a benchmark for text categorization tasks. It works well even with very large volumes of data.
                      </p>
                      
                      <p>
                        Why is Naive Bayes ‚Äúnaive‚Äù, however? It calculates a conditional probability from other individual probabilities, implying independence of features, a fact we‚Äôll almost never encounter in real life.
                      </p>
                      
                      <p>
                        We can infer from looking at the formula that Naive Bayes is supervised üü¢. We need labels in our records to compute the probabilities for values of a certain feature given the label.
                      </p>

                      <h2>Unsupervised Learning üî¥</h2>
                      
                      <h3>Principal Component Analysis (PCA)</h3>

                      <p class="drop-cap">
                        Principal Component Analysis attempts to reduce dimensions in a dataset while preserving as much variance as possible, storing data points in vectors named ‚Äúprincipal components‚Äù. These principal components are in descending order of how much variance in the dataset they carry. Visually, the data points are projected on to the principal axis. Below, PC 1 is the axis that preserves the maximum variance.
                      </p>
                      
                      <p>
                        Finding PC 2 requires a linear <a href="https://en.wikipedia.org/wiki/Orthogonal_transformation">orthogonal transformation</a>. The amount of axes that PCA obtains is equal to the number of dimensions of the dataset so that every principal component is orthogonal to one another.
                      </p>

                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/pca.png" alt="Image" />
                        <p class="wp-caption-text">
                          Image from StatistiXL
                        </p>
                      </div>
                      <!-- image aligncenter -->

                      <p>
                        Step by step, PCA is conducted by computing the covariance matrix, an example of which is shown below. Next, we decompose the covariance matrix to find its <strong><a href="https://www.mathsisfun.com/algebra/eigenvalue.html">eigenvalues</a></strong>, denoted by <i>Œª</i>, and corresponding eigenvectors. Geometrically, the eigenvector points in the resultant direction after the applied transformation, and it is stretched by a factor denoted by the eigenvalue. Therefore, the eigenvector gives us information about the direction of the principal component, while the eigenvalue tells us about its magnitude.
                      </p>
                      
                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/cov.png" alt="Image" />
                        <p class="wp-caption-text">
                          Image from Wikipedia
                        </p>
                      </div>
                      <!-- image aligncenter -->

                      <p>
                        Mathematically, we can express these terms as follows. Consider the linear differential operator, ddx, that scales its eigenvector (or <strong>eigenfunction</strong>). For example, <i>d</i>/<i>dx e</i>^(<i>Œªx</i>)=<i>Œªe</i>^(<i>Œªx</i>). In terms of matrices, let <i>A_n√ón</i> denote a linear transformation, then the eigenvalue equation can be written as a matrix multiplication <i>A<strong>x</strong></i>=<i>Œª<strong>x</strong></i>, where <i><strong>x</strong></i> is the eigenvector. The set of all eigenvectors of a matrix associated with the same eigenvalue (including the zero vector) is called an <strong>eigenspace</strong>.
                      </p>
                      
                      <p>
                        This math might look menacing, but you‚Äôll have a firm understanding after you‚Äôve completed one linear algebra course. It all boils down to geometric intuition.
                      </p>
                      
                      <p>
                        Note that PCA won‚Äôt work well if you:
                        <ol>
                          <li>Don‚Äôt have a <strong>linear relationship</strong> between your variables.</li>
                          <li>Don‚Äôt have a large enough sample size.</li>
                          <li>Don‚Äôt have data suitable for dimensionality reduction.</li>
                          <li>Have significant outliers</li>
                        </ol>
                      </p>
                      
                      <p>
                        PCA is an unsupervised algorithm üî¥. It learns without any target variable. We also tend to associate clustering techniques with unsupervised learning algorithms. People have contested about whether it can be considered a machine learning method at all since it‚Äôs used largely for preprocessing, but you can use the resultant eigenvectors to explain behavior in your data.
                      </p>
                      
                      <h3><i>K</i>-Means</h3>
                      
                      <p class="drop-cap">
                        Clustering excavates a dataset and discovers natural groupings, or clusters, within it, not necessarily disjoint. <i>K</i>-Means accomplishes this by computing the distance from a particular record to fixed numbers called <strong>centroids</strong>. As more records are ‚Äúprocessed‚Äù, the centroids are redefined to equal the means of their corresponding groups. This is essentially the high-level description of the algorithm.
                      </p>
                      
                      <p>
                        Theoretically, <i>K</i>-Means works to minimize its objective function, which is the squared error. The squared error expresses intra-cluster variance.
                      </p>

                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/km-formula.png" alt="Image" />
                        <p class="wp-caption-text">
                          Image created by author
                        </p>
                      </div>
                      <!-- image aligncenter -->
                      
                      <p>
                        <ol>
                          <li><i>J</i> is the objective function.</li>
                          <li><i>k</i> is the predefined number of clusters.</li>
                          <li><i>n</i> is the number of records.</li>
                          <li><i>x_i</i>^(<i>j</i>) is the i-th point in the j-th cluster.</li>
                          <li><i>c_j</i> is the <i>j</i>-th centroid.</li>
                          <li>||<i>x_i</i>^(<i>j</i>) ‚Äî <i>c_j</i>||^2 is the Euclidean distance function.</li>
                        </ol>
                      </p>
                      
                      <p>
                        In practice:
                        <ol>
                          <li>k points are selected at random as cluster centroids.</li>
                          <li>Objects are assigned to a cluster according to its minimum Euclidean distance from each centroid.</li>
                          <li>Update the centroids. They will be recalculated as the mean of the objects in their analogous cluster.</li>
                          <li><strong>Stopping condition</strong>: The centroids do not change, or the objects remain in the same cluster, or the maximum number of iterations is reached.</li>
                        </ol>
                      </p>
                      
                      <p>Below is a useful flowchart that visualizes the k-means algorithm:</p>

                      <!-- image aligncenter -->
                      <div class="wp-caption aligncenter">
                        <img src="images/blog/ml-blog/km.png" alt="Image"/>
                      </div>
                      <!-- image aligncenter -->

                      <p>
                        K-Means clustering is one of the most popular and straightforward unsupervised learning algorithms üî¥. It infers the grouping solely from the Euclidean distance, and not any initial labels. However, there are semi-supervised variations of k-Means, such as <a href="https://arxiv.org/abs/1602.00360">semi-supervised k-means++</a>, that adopt partially labeled datasets to add ‚Äúweights‚Äù to cluster assignment. Much of the time, ‚Äúsupervised‚Äù and ‚Äúunsupervised‚Äù labels shouldn‚Äôt serve as limitations for machine algorithms, and certainly shouldn‚Äôt forestall new developments for alternative methods on the same dataset.
                      </p>
                      
                      <h3>Association Rules</h3>
                      
                      <p class="drop-cap">
                        Finding interesting <strong>associations</strong> among data items is the leading unsupervised learning method after clustering. Association learning algorithms track the rates of complimentary cases in datasets, and make sure that these associations are found after random sampling. This approach is rule-based so it scales to categorical databases. I‚Äôll briefly discuss its motivation and intuition.
                      </p>
                      
                      <p>
                        Consider a database of 10,000 transactions from a store. It is found that 5,000 customers bought Item A and 8,000 bought Item B. Although at first glance, these transactions are statistically independent, but a second look at the dataset helps reveal that 2,000 bought both Items A and B. Not only did 50% of the customers buy Item A and 80% Item B, but 20% bought both items. This information has been useful for marketing strategies.
                      </p>
                      
                      <p>
                        Rules are developed from these uncovered relationships. The composition of an association rule is as follows: an antecedent implies a consequent, and each element in both of these sets makes up an itemset. For example:
                      </p>
                      
                      <p>
                        {<i>A</i>,<i>B</i>}‚áí{<i>C</i>}; <i>I</i>={<i>A</i>,<i>B</i>,<i>C</i>}
                      </p>
                      
                      <p>
                        Various metrics such as <strong>support</strong>, <strong>lift</strong>, and <strong>confidence</strong> help quantify the strengths of found associations. Support is expressed as the frequency of an itemset in a database, as calculated by the following:
                      </p>
                      
                      <p>
                        <i>supp</i>(<i>X</i>)=|{<i>t</i>‚àà<i>T</i>;<i>X</i>‚äÜ<i>t</i>}| / |T|, where <i>T</i> is a database of transactions, and <i>X</i> is the itemset in question.
                      </p>
                      
                      <p>
                        Confidence is the rate at which a rule such as <i>X</i>‚áí<i>Y</i> is found to be true. It‚Äôs expressed as the proportion of transactions that contain <i>X</i> that also contain <i>Y</i>.
                      </p>
                      
                      <p>
                        <i>conf</i>(<i>X</i>‚áí<i>Y</i>)=<i>supp</i>(<i>X</i>‚à™<i>Y</i>) / <i>supp</i>(<i>X</i>)
                      </p>
                      
                      <p>
                        Lift of a rule is the ratio of the actual confidence and the expected confidence. How likely is Item B to be purchased when Item A is purchased, while still adjusting for how popular Item B is? It‚Äôs defined as follows:
                      </p>
                      
                      <p>
                        <i>lift</i>(<i>X</i>‚áí<i>Y</i>)=<i>supp</i>(<i>X</i>‚à™<i>Y</i>) / <i>supp</i>(<i>X</i>)√ó<i>supp</i>(<i>Y</i>)
                      </p>
                      
                      <p>
                        Some known algorithms that discover these rules are the Apriori algorithm, Eclat, and FP-growth. Again, no class labels are assigned to the datasets in use, and it works solely in its constraints to find relationships, so it is unsupervised üî¥.
                      </p>
                      
                      <h2>Conclusion</h2>
                      
                      <p class="drop-cap">
                        I hope you found this article useful in differentiating unsupervised and supervised learning, and what segregates machine learning algorithms from our internal processes of problem solving and building intuition. However, the upper echelons of the artificial intelligence world are rapidly developing, with self-aware agents, deep learning, attention mechanisms, and so much more, aiming to mimic our own cognitive systems.
                      </p>

                      <!-- .entry-meta -->
                      <footer class="entry-meta tags">
                        <a href="#" rel="tag">artificial intelligence</a>
                        <a href="#" rel="tag">machine learning</a>
                        <a href="#" rel="tag">unsupervised learning</a>
                        <a href="#" rel="tag">supervised learning</a>
                        <a href="#" rel="tag">data science</a>
                      </footer>
                      <!-- .entry-meta -->
                    </div>
                    <!-- .entry-content -->
                  </article>
                  <!-- .hentry -->

                  <!-- .nav-single -->
                  <nav class="nav-single row">
                    <div class="nav-previous col-sm-6">
                      <h6>PREVIOUS POST</h6>
                      <h5>
                        <a href="summer-blog.html" rel="prev"
                          ><span class="meta-nav">‚Üê</span> How to Make Your Summer Productive</a
                        >
                      </h5>
                    </div>

                    <div class="nav-next col-sm-6">
                      <h6>NEXT POST</h6>
                      <h5>
                        <a href="python-blog.html" rel="next"
                        >
                        Python, Functional Programming, and Mathematical Computation for Data Science
                          <span class="meta-nav">‚Üí</span></a>
                      </h5>
                    </div>
                  </nav>
                  <!-- .nav-single -->
                </div>
                <!-- .blog-single -->
              </div>
              <!-- site-content -->
            </div>
            <!-- primary -->
          </div>
          <!-- layout -->
        </div>
        <!-- site-middle -->

        <!-- .site-footer -->
        <footer id="colophon" class="site-footer" role="contentinfo">
          <!-- .site-info -->
          <div class="site-info">
            <div class="textwidget">
              <p>
                <a href="index.html"
                  >Copyright &copy; 2020 by Camille Dunning</a
                >
              </p>
            </div>
          </div>
          <!-- .site-info -->
        </footer>
        <!-- .site-footer -->
      </main>
      <!-- .site-main -->

      <!-- ALERT : used for contact form mail delivery alert -->
      <div class="site-alert animated"></div>
    </div>
    <!-- PAGE -->

    <!-- SCRIPTS -->
    <script src="js/fastclick.js"></script>
    <script src="js/jquery.address.js"></script>
    <script src="js/nprogress/nprogress.js"></script>
    <script src="js/jquery.isotope.min.js"></script>
    <script src="js/imagesloaded.pkgd.min.js"></script>
    <script src="js/jquery.fitvids.js"></script>
    <script src="js/jquery.magnific-popup/jquery.magnific-popup.min.js"></script>
    <script src="js/jquery.easing.1.3.js"></script>
    <script src="js/jquery.validate.min.js"></script>

    <script src="https://maps.googleapis.com/maps/api/js"></script>
    <!-- 
        	Get Google Map Api Key
        	https://developers.google.com/maps/documentation/javascript/get-api-key
            
            Get your google map api key and uncomment the script code below and paste your api key.
        -->
    <!--<script src="https://maps.googleapis.com/maps/api/js?key=YOUR_API_KEY_HERE"></script>-->

    <script src="js/main.js"></script>
  </body>
</html>
